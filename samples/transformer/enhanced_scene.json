{
  "scenes": [
    {
      "scene_parts": [
        {
          "animation_details": "Display the text \"RNN Limitations\" in white, centered at the top with a size of 0.8. Below it, position a simple RNN diagram with nodes labeled \"Input\", \"Hidden\", and \"Output\". The input sequence \"A B C D\" is on the left, with each token connected by red arrows to show the sequential flow. Animate the processing of each token one at a time, moving from left to right. Place a grey clock icon (0.5 size) on the right, below the \"RNN Limitations\" text but above the limitations text. Below the clock icon, display the text \"Vanishing/Exploding Gradients\" and \"Difficulty with Long-Range Dependencies\" in white, each on a new line. Make sure text doesn't overlap",
          "animation_cleaning": "No objects should be cleared",
          "audio_text": "Before transformers, we had Recurrent Neural Networks, or RNNs. They process sequences one token at a time, which is quite slow. They also struggle with long-range dependencies and gradients.",
          "estimated_audio_duration_s": 12
        },
        {
          "animation_details": "Fade out the RNN diagram and the associated text. Display the text \"Introducing Transformers\" in white, centered at the top with size 0.8. Below it, show a basic Transformer diagram. Use multiple nodes to represent the components and green parallel arrows to indicate parallel processing. Label the center of the diagram with \"Attention Mechanism\" in white with size 0.7. Make sure that diagram is not overlapping any text.",
          "animation_cleaning": "Clear all objects",
          "audio_text": "Transformers, introduced in the paper \"Attention is All You Need\", solve these issues with parallel processing and a powerful attention mechanism.",
          "estimated_audio_duration_s": 8
        }
      ],
      "scene_style": "Dark blue background. White and yellow for text. RNN diagram with red arrows for sequential flow. Clock icon in grey. Transformer diagram with green parallel arrows. Smooth transitions between text and diagrams. Show equations in standard blue.",
      "id": 1
    },
    {
      "scene_parts": [
        {
          "animation_details": "Display the text \"Input: The quick brown fox\" in white at the top left with size 0.7. Below each word, show its transformation into a vector: \"The\" -> [0.1, 0.2, 0.3], \"quick\" -> [0.4, 0.5, 0.6], \"brown\" -> [0.2, 0.3, 0.5], \"fox\" -> [0.6, 0.1, 0.4]. Use blue for the vectors. Display the text \"Embeddings\" in white at the middle with a size of 0.8.",
          "animation_cleaning": "No objects should be cleared",
          "audio_text": "First, we convert words into numerical representations called embeddings.",
          "estimated_audio_duration_s": 4
        },
        {
          "animation_details": "Display the text \"Positional Encoding\" in white at the top with a size of 0.8. Below it, write the equations `PE(pos, 2i) = sin(pos / 10000^(2i/d_model))` and `PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))` in yellow with size 0.6. Show a matrix of embeddings in blue. Animate adding positional vectors to these embeddings, illustrating the matrix addition at the bottom of the screen.",
          "animation_cleaning": "Clear all objects",
          "audio_text": "Since transformers don't process sequentially, we add positional encoding to give the model information about the order of words. This is done using sine and cosine functions, as shown in these equations.",
          "estimated_audio_duration_s": 13
        }
      ],
      "scene_style": "Dark blue background. White text for \"Input\", \"Embeddings\", and \"Positional Encoding\". Use blue for word vectors and embeddings. Equations in yellow. Smooth transitions between text, vectors, and equations.",
      "id": 2
    },
    {
      "scene_parts": [
        {
          "animation_details": "Display the text \"Multi-Head Self-Attention\" in white, centered at the top with a size of 0.8.",
          "animation_cleaning": "No objects should be cleared",
          "audio_text": "The core of the transformer is the multi-head self-attention.",
          "estimated_audio_duration_s": 4
        },
        {
          "animation_details": "Show input embeddings matrix X in blue on the left. Display the linear transformations: `Q = X * W_Q`, `K = X * W_K`, `V = X * W_V` in standard blue, centered on the screen. Show matrices W_Q, W_K, W_V next to the equations. Ensure equations and matrices are not overlapping and are well within the screen's bounds.",
          "animation_cleaning": "Clear all objects",
          "audio_text": "We derive Query, Key, and Value matrices from the input embeddings.",
          "estimated_audio_duration_s": 4
        },
        {
          "animation_details": "Display the formula `Attention(Q, K, V) = softmax((Q * K^T) / sqrt(d_k)) * V` in standard blue, centered on the screen with size 0.7.",
          "animation_cleaning": "Clear all objects",
          "audio_text": "Each head calculates attention using the scaled dot-product formula.",
          "estimated_audio_duration_s": 4
        },
        {
          "animation_details": "Show three attention heads at the top, each with different Q, K, V matrices in red, green, and yellow, respectively. Label each head clearly. Animate the outputs of each head being concatenated at the bottom. Show a final linear transformation of the concatenated output at the bottom right. Use arrows to indicate the flow from individual heads to concatenation and then to the final linear transformation.",
          "animation_cleaning": "Clear all objects",
          "audio_text": "Multiple heads work in parallel, capturing different relationships, and their outputs are concatenated and linearly transformed.",
          "estimated_audio_duration_s": 6
        }
      ],
      "scene_style": "Dark blue background. White text for \"Multi-Head Self-Attention\". Blue for matrices and equations. Use different colors for each attention head (red, green, yellow). Smooth transitions between matrix operations.",
      "id": 3
    },
    {
      "scene_parts": [
        {
          "animation_details": "Display the text \"Feed-Forward Network\" in white, centered at the top with a size of 0.8. Below it, show a simple two-layer neural network diagram with input, hidden, and output layers. Use blue nodes and green connections. Animate data flowing through the network with arrows, starting from the input layer, moving to the hidden layer, and finally to the output layer. keep size of diagram such that it is not overlapping with text.",
          "animation_cleaning": "No objects should be cleared",
          "audio_text": "Each encoder and decoder layer contains a feed-forward network, which is a simple two-layer neural network.",
          "estimated_audio_duration_s": 6
        },
        {
          "animation_details": "Display the text \"Residual Connections\" in white, centered at the top with size 0.8. Below it, show the addition: `Layer Input + Sublayer(Layer Input)` in standard blue with size 0.7. Highlight the plus sign in red.",
          "animation_cleaning": "Clear all objects",
          "audio_text": "Residual connections add the input of each sub-layer to its output, aiding gradient flow.",
          "estimated_audio_duration_s": 6
        },
        {
          "animation_details": "Display the text \"Layer Normalization\" in white, centered at the top with a size of 0.8. Below it, show the formula `LayerNorm(x + Sublayer(x))` in yellow with size 0.7.",
          "animation_cleaning": "Clear all objects",
          "audio_text": "Layer normalization is applied to stabilize training.",
          "estimated_audio_duration_s": 3
        }
      ],
      "scene_style": "Dark blue background. White text for titles. Neural network diagram with blue nodes and green connections. Residual connection shown with a plus sign in red. Layer normalization equation in yellow.",
      "id": 4
    },
    {
      "scene_parts": [
        {
          "animation_details": "Show the complete transformer diagram with encoder and decoder blocks, centered on the screen. Use blue for the encoder and green for the decoder. Animate data flow from input embeddings through the encoder layers to the intermediate representation. Show data flow through the decoder layers, highlighting masked multi-head attention and encoder-decoder attention with white arrows. Make sure diagram doesn't overlap.",
          "animation_cleaning": "No objects should be cleared",
          "audio_text": "Here's the complete transformer structure. Data flows through the encoder, gets transformed, and then passes to the decoder.",
          "estimated_audio_duration_s": 7
        },
        {
          "animation_details": "Show the final linear layer in yellow at the bottom right. Connect it to the output of the decoder. Represent the softmax function in red. Display the output probabilities in light blue, connected to the softmax function. Animate the flow from the decoder output to the linear layer, then to the softmax function, and finally to the output probabilities.",
          "animation_cleaning": "Clear all objects",
          "audio_text": "The decoder uses multi-head attention and, finally, a linear layer with a softmax function produces output probabilities.",
          "estimated_audio_duration_s": 7
        }
      ],
      "scene_style": "Dark blue background. Use consistent colors from previous scenes. Encoder in blue, decoder in green. Data flow arrows in white. Final linear layer in yellow, softmax function in red. Output probabilities in light blue.",
      "id": 5
    },
    {
      "scene_parts": [
        {
          "animation_details": "Display the text \"Advantages of Transformers\" in white, centered at the top with a size of 0.8. Below it, list \"Parallelization\", \"Long-Range Dependencies\", \"Transfer Learning\" in green, each on a new line. On the right, show icons for NLP (text), computer vision (image), and other fields (speech, bioinformatics) in light blue. Each icon should be distinct and recognizable, sized at 0.5. Make sure that text and icon are not overlapping.",
          "animation_cleaning": "No objects should be cleared",
          "audio_text": "Transformers offer significant advantages, including parallel processing, handling long-range dependencies, and enabling transfer learning.",
          "estimated_audio_duration_s": 6
        },
        {
          "animation_details": "Display the text \"Applications\" in white, centered at the top with a size of 0.8. Below it, list application examples: \"Machine Translation\", \"Text Summarization\", \"Image Classification\" in yellow, each on a new line and size of 0.7. Ensure the text is legible and does not overlap with other elements.",
          "animation_cleaning": "Clear all objects",
          "audio_text": "They have revolutionized various fields, including machine translation, text summarization, and image classification. The transformer model has truly transformed machine learning.",
          "estimated_audio_duration_s": 8
        }
      ],
      "scene_style": "Dark blue background. White text for \"Advantages of Transformers\" and \"Applications\". Use green for advantage points and yellow for application examples. Icons in light blue. Smooth transitions between text and icons.",
      "id": 6
    }
  ]
}